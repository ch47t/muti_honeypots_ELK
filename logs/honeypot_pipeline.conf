# -------------------------------------------------------------------------
# INPUT STAGE: Define a separate input for each honeypot log file
# -------------------------------------------------------------------------
input {
    # 1. FTP Honeypot Logs
    file {
        path => "/home/kali/honeypots/logs/ftp.json" 
        start_position => "beginning"
        codec => json
        type => "ftp_honeypot" # Unique identifier for filtering
    }

    # 2. HTTP Honeypot Logs
    file {
        path => "/home/kali/honeypots/logs/http.json" 
        start_position => "beginning"
        codec => json
        type => "http_honeypot" 
    }

    # 3. SSH Honeypot Logs
    file {
        path => "/home/kali/honeypots/logs/ssh.json" 
        start_position => "beginning"
        codec => json
        type => "ssh_honeypot"
    }
}

# -------------------------------------------------------------------------
# FILTER STAGE: Common parsing and field standardization
# -------------------------------------------------------------------------
filter {
    
    date {
        match => ["timestamp", "ISO8601"] # Since all your honeypots log a 'timestamp' field in ISO8601 format
        target => "@timestamp"            # Map the string field to the Kibana standard
        remove_field => ["timestamp"]     # Remove the original string field
    }
    
    # --- General Cleanup ---
    mutate {
        # Remove fields added by the file input that are often unneeded
        remove_field => ["@version", "host", "path"]
    }
}

# -------------------------------------------------------------------------
# OUTPUT STAGE: Send all processed logs to Elasticsearch
# -------------------------------------------------------------------------
output {
    elasticsearch {
        hosts => ["https://127.0.0.1:9200"] # <-- Note the 'https'
        index => "honeypot-logs-%{+YYYY.MM.dd}"
        user => "elastic"
        password => "MdjS0waAtSM*66rmu4B+" 
        ssl_verification_mode => "none"
    }
}
